{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasongan1028-cmd/Halo/blob/main/Milestone_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Milestone 1: A Spatiotemporal Analysis of Socioeconomic and Demographic Correlates in Walmart Supercenter Closures (2019-2024)**\n"
      ],
      "metadata": {
        "id": "283DLy7mi5wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Team Members: Jason Gan (jasongan), Tingting Zhu (zhutt), Xiaojun Xu (xiaojunx) \\\n",
        "Mentor: Erik Lang"
      ],
      "metadata": {
        "id": "L3akSut-jPl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Up Environment**"
      ],
      "metadata": {
        "id": "YmN8s35Oi7Bn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk4jzR-BdRrL",
        "outputId": "5e51cc6c-b9e3-4176-c241-70affe2b96d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BgH3MXAub2io"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "V8z4slducNUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843eca78-2114-45eb-c714-bb7202dd3372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/walmart_OSM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAM4viqScDxH",
        "outputId": "4deae348-8d8c-44d1-bff4-74e613c70d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'B01003 Total population.csv'\t\t     walmart_2024_geocode.csv\n",
            "'B03002 Hispanic or Latino population.csv'   walmart_acs_master.csv\n",
            "'B19013 Median household income.csv'\t     walmart_closure_comparison.csv\n",
            " raw_2024_by_state\t\t\t     walmart_closure_with_acs.csv\n",
            " walmart_2024_all_states.csv\t\t     walmart_Kaggle.csv\n",
            " walmart_2024_all_states.json\t\t    'walmart_Kaggle_final .csv'\n",
            " walmart_2024_final.csv\t\t\t     walmart_Kaggle_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpX5MZvdp-fy"
      },
      "source": [
        "# **Primary Data: Walmart Closures During 2019-2024**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7JiRTs9KHrw"
      },
      "source": [
        "## **1. Data Extraction: Open Street Map API (2024)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVzOJOePP3lP",
        "outputId": "d4e89d56-6276-4da5-df63-df615af4c5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Pre-check existing JSON files ====\n",
            "\n",
            "Alabama              | OK (199 items)                      → WILL SKIP\n",
            "Alaska               | OK (11 items)                       → WILL SKIP\n",
            "Arizona              | OK (219 items)                      → WILL SKIP\n",
            "Arkansas             | OK (184 items)                      → WILL SKIP\n",
            "California           | OK (431 items)                      → WILL SKIP\n",
            "Colorado             | OK (179 items)                      → WILL SKIP\n",
            "Connecticut          | OK (42 items)                       → WILL SKIP\n",
            "Delaware             | OK (10 items)                       → WILL SKIP\n",
            "District of Columbia | OK (2 items)                        → WILL SKIP\n",
            "Florida              | OK (515 items)                      → WILL SKIP\n",
            "Georgia              | OK (296 items)                      → WILL SKIP\n",
            "Hawaii               | OK (12 items)                       → WILL SKIP\n",
            "Idaho                | OK (53 items)                       → WILL SKIP\n",
            "Illinois             | OK (237 items)                      → WILL SKIP\n",
            "Indiana              | OK (160 items)                      → WILL SKIP\n",
            "Iowa                 | OK (82 items)                       → WILL SKIP\n",
            "Kansas               | OK (110 items)                      → WILL SKIP\n",
            "Kentucky             | OK (130 items)                      → WILL SKIP\n",
            "Louisiana            | OK (180 items)                      → WILL SKIP\n",
            "Maine                | OK (43 items)                       → WILL SKIP\n",
            "Maryland             | OK (68 items)                       → WILL SKIP\n",
            "Massachusetts        | OK (65 items)                       → WILL SKIP\n",
            "Michigan             | OK (121 items)                      → WILL SKIP\n",
            "Minnesota            | OK (101 items)                      → WILL SKIP\n",
            "Mississippi          | OK (114 items)                      → WILL SKIP\n",
            "Missouri             | OK (202 items)                      → WILL SKIP\n",
            "Montana              | OK (36 items)                       → WILL SKIP\n",
            "Nebraska             | OK (72 items)                       → WILL SKIP\n",
            "Nevada               | OK (69 items)                       → WILL SKIP\n",
            "New Hampshire        | OK (47 items)                       → WILL SKIP\n",
            "New Jersey           | OK (76 items)                       → WILL SKIP\n",
            "New Mexico           | OK (77 items)                       → WILL SKIP\n",
            "New York             | OK (146 items)                      → WILL SKIP\n",
            "North Carolina       | OK (274 items)                      → WILL SKIP\n",
            "North Dakota         | OK (16 items)                       → WILL SKIP\n",
            "Ohio                 | OK (265 items)                      → WILL SKIP\n",
            "Oklahoma             | OK (195 items)                      → WILL SKIP\n",
            "Oregon               | OK (105 items)                      → WILL SKIP\n",
            "Pennsylvania         | OK (238 items)                      → WILL SKIP\n",
            "Rhode Island         | OK (16 items)                       → WILL SKIP\n",
            "South Carolina       | OK (172 items)                      → WILL SKIP\n",
            "South Dakota         | OK (29 items)                       → WILL SKIP\n",
            "Tennessee            | OK (207 items)                      → WILL SKIP\n",
            "Texas                | OK (860 items)                      → WILL SKIP\n",
            "Utah                 | OK (102 items)                      → WILL SKIP\n",
            "Vermont              | OK (7 items)                        → WILL SKIP\n",
            "Virginia             | OK (218 items)                      → WILL SKIP\n",
            "Washington           | OK (139 items)                      → WILL SKIP\n",
            "West Virginia        | OK (56 items)                       → WILL SKIP\n",
            "Wisconsin            | OK (134 items)                      → WILL SKIP\n",
            "Wyoming              | OK (27 items)                       → WILL SKIP\n"
          ]
        }
      ],
      "source": [
        "|# check skip-retry\n",
        "OUT_DIR = \"/content/drive/MyDrive/walmart_OSM/raw_2024_by_state\"\n",
        "\n",
        "US_STATES = {\n",
        "    \"Alabama\": \"US-AL\", \"Alaska\": \"US-AK\", \"Arizona\": \"US-AZ\", \"Arkansas\": \"US-AR\",\n",
        "    \"California\": \"US-CA\", \"Colorado\": \"US-CO\", \"Connecticut\": \"US-CT\", \"Delaware\": \"US-DE\",\n",
        "    \"District of Columbia\": \"US-DC\",\"Florida\": \"US-FL\",\"Georgia\": \"US-GA\", \"Hawaii\": \"US-HI\",\n",
        "    \"Idaho\": \"US-ID\", \"Illinois\": \"US-IL\", \"Indiana\": \"US-IN\", \"Iowa\": \"US-IA\",\n",
        "    \"Kansas\": \"US-KS\", \"Kentucky\": \"US-KY\", \"Louisiana\": \"US-LA\", \"Maine\": \"US-ME\",\n",
        "    \"Maryland\": \"US-MD\", \"Massachusetts\": \"US-MA\", \"Michigan\": \"US-MI\", \"Minnesota\": \"US-MN\",\n",
        "    \"Mississippi\": \"US-MS\", \"Missouri\": \"US-MO\", \"Montana\": \"US-MT\", \"Nebraska\": \"US-NE\",\n",
        "    \"Nevada\": \"US-NV\", \"New Hampshire\": \"US-NH\", \"New Jersey\": \"US-NJ\", \"New Mexico\": \"US-NM\",\n",
        "    \"New York\": \"US-NY\", \"North Carolina\": \"US-NC\", \"North Dakota\": \"US-ND\", \"Ohio\": \"US-OH\",\n",
        "    \"Oklahoma\": \"US-OK\", \"Oregon\": \"US-OR\", \"Pennsylvania\": \"US-PA\", \"Rhode Island\": \"US-RI\",\n",
        "    \"South Carolina\": \"US-SC\", \"South Dakota\": \"US-SD\", \"Tennessee\": \"US-TN\", \"Texas\": \"US-TX\",\n",
        "    \"Utah\": \"US-UT\", \"Vermont\": \"US-VT\", \"Virginia\": \"US-VA\", \"Washington\": \"US-WA\",\n",
        "    \"West Virginia\": \"US-WV\", \"Wisconsin\": \"US-WI\", \"Wyoming\": \"US-WY\"\n",
        "}\n",
        "\n",
        "def state_outfile(iso_code):\n",
        "    return os.path.join(OUT_DIR, f\"walmart_{iso_code}_2024.json\")\n",
        "\n",
        "\n",
        "def check_file(path):\n",
        "    if not os.path.exists(path):\n",
        "        return \"MISSING\"\n",
        "\n",
        "    try:\n",
        "        size = os.path.getsize(path)\n",
        "        if size == 0:\n",
        "            return \"EMPTY FILE\"\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if not isinstance(data, list):\n",
        "            return \"NOT A LIST\"\n",
        "\n",
        "        if len(data) == 0:\n",
        "            return \"EMPTY LIST\"\n",
        "\n",
        "        return f\"OK ({len(data)} items)\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"CORRUPTED JSON: {e}\"\n",
        "\n",
        "\n",
        "print(\"==== Pre-check existing JSON files ====\\n\")\n",
        "\n",
        "for state_name, iso_code in US_STATES.items():\n",
        "    path = state_outfile(iso_code)\n",
        "    status = check_file(path)\n",
        "\n",
        "    if status.startswith(\"OK\"):\n",
        "        action = \"→ WILL SKIP\"\n",
        "    else:\n",
        "        action = \"→ WILL RE-FETCH\"\n",
        "\n",
        "    print(f\"{state_name:20s} | {status:35s} {action}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_QdRjxJziUI",
        "outputId": "97e86351-43e2-4741-d629-e5919ef609a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching FL block 1/4 bbox=(24.4,-87.7,27.75,-83.85)\n",
            "  Block 1 got 0 elements.\n",
            "Fetching FL block 2/4 bbox=(24.4,-83.85,27.75,-80.0)\n",
            "  Block 2 got 177 elements.\n",
            "Fetching FL block 3/4 bbox=(27.75,-87.7,31.1,-83.85)\n",
            "  Block 3 got 54 elements.\n",
            "Fetching FL block 4/4 bbox=(27.75,-83.85,31.1,-80.0)\n",
            "  Block 4 got 284 elements.\n",
            "\n",
            "Florida total after dedup: 515\n",
            "Saved: /content/drive/MyDrive/walmart_OSM/raw_2024_by_state/walmart_US-FL_2024.json\n"
          ]
        }
      ],
      "source": [
        "# Code3: FL（split to 4 bbox)\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "ISO_CODE = \"US-FL\"\n",
        "STATE_NAME = \"Florida\"\n",
        "DATE_ISO = \"2024-12-31T23:59:59Z\"\n",
        "\n",
        "OVERPASS_URLS = [\n",
        "    \"https://overpass-api.de/api/interpreter\",\n",
        "    \"https://overpass.kumi.systems/api/interpreter\",\n",
        "]\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"walmart-research/1.0 (contact: zhutt@umich.edu)\"}\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/walmart_OSM/raw_2024_by_state\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "OUT_PATH = os.path.join(OUT_DIR, f\"walmart_{ISO_CODE}_2024.json\")\n",
        "\n",
        "# Florida rough bounding box, split into 2x2 (4 queries)\n",
        "# (minlat, minlon, maxlat, maxlon)\n",
        "FL_BOXES = [\n",
        "    (24.4, -87.7, 27.75, -83.85),\n",
        "    (24.4, -83.85, 27.75, -80.0),\n",
        "    (27.75, -87.7, 31.1, -83.85),\n",
        "    (27.75, -83.85, 31.1, -80.0),\n",
        "]\n",
        "\n",
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "def atomic_write_json(path: str, data):\n",
        "    tmp = path + \".tmp\"\n",
        "    with open(tmp, \"w\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def dedup_elements(elems):\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for e in elems:\n",
        "        k = (e.get(\"type\"), e.get(\"id\"))\n",
        "        if k in seen:\n",
        "            continue\n",
        "        seen.add(k)\n",
        "        out.append(e)\n",
        "    return out\n",
        "\n",
        "def build_query_bbox(minlat, minlon, maxlat, maxlon, date_iso=DATE_ISO):\n",
        "    # Your original matching logic, applied to bbox instead of state area\n",
        "    return f\"\"\"\n",
        "    [out:json][timeout:1800][date:\"{date_iso}\"];\n",
        "    (\n",
        "      nwr({minlat},{minlon},{maxlat},{maxlon})[\"brand\"=\"Walmart\"];\n",
        "      nwr({minlat},{minlon},{maxlat},{maxlon})[\"name\"~\"^Walmart\",i];\n",
        "    );\n",
        "    out center tags;\n",
        "    \"\"\"\n",
        "\n",
        "def run_overpass(query: str, max_tries: int = 10):\n",
        "    last_err = None\n",
        "    for attempt in range(max_tries):\n",
        "        url = random.choice(OVERPASS_URLS)\n",
        "        try:\n",
        "            r = requests.post(\n",
        "                url,\n",
        "                data={\"data\": query},\n",
        "                headers=HEADERS,\n",
        "                timeout=(30, 1900)  # connect, read\n",
        "            )\n",
        "\n",
        "            if r.status_code == 200:\n",
        "                try:\n",
        "                    return r.json().get(\"elements\", [])\n",
        "                except Exception as e:\n",
        "                    last_err = f\"JSON decode error @ {url}: {repr(e)}\"\n",
        "\n",
        "            elif r.status_code in (429, 502, 503, 504):\n",
        "                last_err = f\"HTTP {r.status_code} @ {url}\"\n",
        "\n",
        "            else:\n",
        "                last_err = f\"HTTP {r.status_code} @ {url}: {r.text[:200]}\"\n",
        "                print(\"[NON-RETRY]\", last_err)\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = f\"EXC @ {url}: {repr(e)}\"\n",
        "\n",
        "        sleep_s = min(240, (2 ** attempt) + random.random() * 10)\n",
        "        print(f\"[RETRY] {last_err} | sleep {sleep_s:.1f}s\")\n",
        "        time.sleep(sleep_s)\n",
        "\n",
        "    print(f\"[FAIL] After {max_tries} tries. Last error: {last_err}\")\n",
        "    return None\n",
        "\n",
        "# -----------------------\n",
        "# Fetch Florida by bbox blocks\n",
        "# -----------------------\n",
        "all_elems = []\n",
        "for i, (minlat, minlon, maxlat, maxlon) in enumerate(FL_BOXES, start=1):\n",
        "    print(f\"Fetching FL block {i}/{len(FL_BOXES)} bbox=({minlat},{minlon},{maxlat},{maxlon})\")\n",
        "    q = build_query_bbox(minlat, minlon, maxlat, maxlon)\n",
        "    elems = run_overpass(q, max_tries=12)\n",
        "\n",
        "    if elems is None:\n",
        "        raise RuntimeError(f\"Florida block {i} failed. Try increasing split (3x3) or removing [date:].\")\n",
        "\n",
        "    # add state tag\n",
        "    for item in elems:\n",
        "        if isinstance(item, dict):\n",
        "            tags = item.get(\"tags\")\n",
        "            if isinstance(tags, dict):\n",
        "                tags[\"state_ref\"] = STATE_NAME\n",
        "\n",
        "    all_elems.extend(elems)\n",
        "    print(f\"  Block {i} got {len(elems)} elements.\")\n",
        "    time.sleep(6)  # be nice\n",
        "\n",
        "# Dedup + Save\n",
        "all_elems = dedup_elements(all_elems)\n",
        "print(f\"\\nFlorida total after dedup: {len(all_elems)}\")\n",
        "\n",
        "atomic_write_json(OUT_PATH, all_elems)\n",
        "print(f\"Saved: {OUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems:** National-scale queries for Walmart locations consistently triggered API timeouts and memory limits on the Overpass servers, while network instability risked corrupting data saved directly to cloud storage.\n",
        "\n",
        "**Solutions:** We engineered a partitioned extraction pipeline that iterates by state, utilizing Exponential Backoff for retries and Atomic Writing for file saving. Segmenting the data ensures we bypass server-side \"kills,\" while atomic operations and checkpointing allow the script to resume safely after interruptions without losing progress or corrupting the existing dataset.\n",
        "\n",
        "**Expectations:** A complete, verified snapshot of 2024 Walmart locations across 51 jurisdictions, structured for a precise longitudinal merge with 2019 baseline data."
      ],
      "metadata": {
        "id": "iI9o6LD-vsR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUk48wrZ14DW",
        "outputId": "4a2e6eba-9026-4e70-9300-50a5db6f3f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skip Alabama (exists): 199\n",
            "Skip Alaska (exists): 11\n",
            "Skip Arizona (exists): 219\n",
            "Skip Arkansas (exists): 184\n",
            "Skip California (exists): 431\n",
            "Skip Colorado (exists): 179\n",
            "Skip Connecticut (exists): 42\n",
            "Skip Delaware (exists): 10\n",
            "Skip District of Columbia (exists): 2\n",
            "Skip Florida (exists): 515\n",
            "Skip Georgia (exists): 296\n",
            "Skip Hawaii (exists): 12\n",
            "Skip Idaho (exists): 53\n",
            "Skip Illinois (exists): 237\n",
            "Skip Indiana (exists): 160\n",
            "Skip Iowa (exists): 82\n",
            "Skip Kansas (exists): 110\n",
            "Skip Kentucky (exists): 130\n",
            "Skip Louisiana (exists): 180\n",
            "Skip Maine (exists): 43\n",
            "Skip Maryland (exists): 68\n",
            "Skip Massachusetts (exists): 65\n",
            "Skip Michigan (exists): 121\n",
            "Skip Minnesota (exists): 101\n",
            "Skip Mississippi (exists): 114\n",
            "Skip Missouri (exists): 202\n",
            "Skip Montana (exists): 36\n",
            "Skip Nebraska (exists): 72\n",
            "Skip Nevada (exists): 69\n",
            "Skip New Hampshire (exists): 47\n",
            "Skip New Jersey (exists): 76\n",
            "Skip New Mexico (exists): 77\n",
            "Skip New York (exists): 146\n",
            "Skip North Carolina (exists): 274\n",
            "Skip North Dakota (exists): 16\n",
            "Skip Ohio (exists): 265\n",
            "Skip Oklahoma (exists): 195\n",
            "Skip Oregon (exists): 105\n",
            "Skip Pennsylvania (exists): 238\n",
            "Skip Rhode Island (exists): 16\n",
            "Skip South Carolina (exists): 172\n",
            "Skip South Dakota (exists): 29\n",
            "Skip Tennessee (exists): 207\n",
            "Skip Texas (exists): 860\n",
            "Skip Utah (exists): 102\n",
            "Skip Vermont (exists): 7\n",
            "Skip Virginia (exists): 218\n",
            "Skip Washington (exists): 139\n",
            "Skip West Virginia (exists): 56\n",
            "Skip Wisconsin (exists): 134\n",
            "Skip Wyoming (exists): 27\n",
            "\n",
            "FETCH PHASE DONE.\n",
            "Skipped states: 51\n",
            "Retried states: 0\n",
            "Total saved (counted from files + new fetch): 7349\n",
            "FAILED states: []\n"
          ]
        }
      ],
      "source": [
        "# Portions of this code were generated with the assistance of OpenAI GPT-4\n",
        "\n",
        "# Summary of Key Prompts: To optimize the Walmart 2024 extraction script, we focused on three critical failure points:\n",
        "# 1. Iterative Reliability: \"How can we redesign the script to ensure it successfully iterates through all 50 states without timing out or being blocked by the server?\"\n",
        "# 2. Resilience Strategy: \"How can we handle HTTP 429 and 504 errors from the Overpass API?\"\n",
        "# 3. Data Integrity: \"How can we prevent data corruption when saving progress to a cloud-synced directory?\"\n",
        "\n",
        "# Generated on: February 2026\n",
        "\n",
        "US_STATES = {\n",
        "    \"Alabama\": \"US-AL\", \"Alaska\": \"US-AK\", \"Arizona\": \"US-AZ\", \"Arkansas\": \"US-AR\",\n",
        "    \"California\": \"US-CA\", \"Colorado\": \"US-CO\", \"Connecticut\": \"US-CT\", \"Delaware\": \"US-DE\",\n",
        "    \"District of Columbia\": \"US-DC\", \"Florida\": \"US-FL\", \"Georgia\": \"US-GA\", \"Hawaii\": \"US-HI\",\n",
        "    \"Idaho\": \"US-ID\", \"Illinois\": \"US-IL\", \"Indiana\": \"US-IN\", \"Iowa\": \"US-IA\",\n",
        "    \"Kansas\": \"US-KS\", \"Kentucky\": \"US-KY\", \"Louisiana\": \"US-LA\", \"Maine\": \"US-ME\",\n",
        "    \"Maryland\": \"US-MD\", \"Massachusetts\": \"US-MA\", \"Michigan\": \"US-MI\", \"Minnesota\": \"US-MN\",\n",
        "    \"Mississippi\": \"US-MS\", \"Missouri\": \"US-MO\", \"Montana\": \"US-MT\", \"Nebraska\": \"US-NE\",\n",
        "    \"Nevada\": \"US-NV\", \"New Hampshire\": \"US-NH\", \"New Jersey\": \"US-NJ\", \"New Mexico\": \"US-NM\",\n",
        "    \"New York\": \"US-NY\", \"North Carolina\": \"US-NC\", \"North Dakota\": \"US-ND\", \"Ohio\": \"US-OH\",\n",
        "    \"Oklahoma\": \"US-OK\", \"Oregon\": \"US-OR\", \"Pennsylvania\": \"US-PA\", \"Rhode Island\": \"US-RI\",\n",
        "    \"South Carolina\": \"US-SC\", \"South Dakota\": \"US-SD\", \"Tennessee\": \"US-TN\", \"Texas\": \"US-TX\",\n",
        "    \"Utah\": \"US-UT\", \"Vermont\": \"US-VT\", \"Virginia\": \"US-VA\", \"Washington\": \"US-WA\",\n",
        "    \"West Virginia\": \"US-WV\", \"Wisconsin\": \"US-WI\", \"Wyoming\": \"US-WY\"\n",
        "}\n",
        "\n",
        "OVERPASS_URLS = [\n",
        "  \"https://overpass-api.de/api/interpreter\",\n",
        "  \"https://overpass.kumi.systems/api/interpreter\",\n",
        "]\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"walmart-research/1.0 (contact: zhutt@umich.edu)\"}\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/walmart_OSM/raw_2024_by_state\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def state_outfile(iso_code: str) -> str:\n",
        "    return os.path.join(OUT_DIR, f\"walmart_{iso_code}_2024.json\")\n",
        "\n",
        "def build_query(state_name: str, iso_code: str, date_iso: str) -> str:\n",
        "    return f\"\"\"\n",
        "    [out:json][timeout:1800][date:\"{date_iso}\"];\n",
        "    rel[\"name\"=\"{state_name}\"][\"boundary\"=\"administrative\"][\"admin_level\"=\"4\"][\"ISO3166-2\"=\"{iso_code}\"];\n",
        "    map_to_area->.st;\n",
        "    (\n",
        "      nwr[\"brand\"=\"Walmart\"](area.st);\n",
        "      nwr[\"name\"~\"^Walmart\",i](area.st);\n",
        "    );\n",
        "    out center tags;\n",
        "    \"\"\"\n",
        "\n",
        "def load_state_file_if_ok(out_path: str):\n",
        "    \"\"\"Return list if file exists and is a non-empty list; else None.\"\"\"\n",
        "    if not os.path.exists(out_path):\n",
        "        return None\n",
        "    try:\n",
        "        with open(out_path, \"r\") as f:\n",
        "            elems = json.load(f)\n",
        "        if isinstance(elems, list) and len(elems) > 0:\n",
        "            return elems\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def atomic_write_json(path: str, data):\n",
        "    \"\"\"Write JSON safely: write to tmp then replace.\"\"\"\n",
        "    tmp = path + \".tmp\"\n",
        "    with open(tmp, \"w\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def get_walmarts_by_state(state_name: str, iso_code: str,\n",
        "                         date_iso: str = \"2024-12-31T23:59:59Z\",\n",
        "                         max_tries: int = 10):\n",
        "    query = build_query(state_name, iso_code, date_iso)\n",
        "    last_err = None\n",
        "\n",
        "    for attempt in range(max_tries):\n",
        "        url = random.choice(OVERPASS_URLS)\n",
        "\n",
        "        try:\n",
        "            r = requests.post(\n",
        "                url,\n",
        "                data={\"data\": query},\n",
        "                headers=HEADERS,\n",
        "                timeout=(30, 1900)  # connect timeout, read timeout\n",
        "            )\n",
        "\n",
        "            if r.status_code == 200:\n",
        "                try:\n",
        "                    return r.json().get(\"elements\", [])\n",
        "                except Exception as e:\n",
        "                    last_err = f\"JSON decode error @ {url}: {repr(e)}\"\n",
        "                    sleep_s = min(180, (2 ** attempt) + random.random() * 5)\n",
        "                    print(f\"  {state_name}: {last_err}, retry in {sleep_s:.1f}s\")\n",
        "                    time.sleep(sleep_s)\n",
        "                    continue\n",
        "\n",
        "            if r.status_code in (429, 502, 503, 504):\n",
        "                last_err = f\"HTTP {r.status_code} @ {url}\"\n",
        "                sleep_s = min(180, (2 ** attempt) + random.random() * 5)\n",
        "                print(f\"  {state_name}: {last_err}, retry in {sleep_s:.1f}s\")\n",
        "                time.sleep(sleep_s)\n",
        "                continue\n",
        "\n",
        "            last_err = f\"HTTP {r.status_code} @ {url}: {r.text[:120]}\"\n",
        "            print(f\"  {state_name}: {last_err}\")\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = f\"EXC @ {url}: {repr(e)}\"\n",
        "            sleep_s = min(180, (2 ** attempt) + random.random() * 5)\n",
        "            print(f\"  {state_name}: {last_err}, retry in {sleep_s:.1f}s\")\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "    print(f\"[FAIL] {state_name} after {max_tries} tries. Last error: {last_err}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# FETCH with checkpointing\n",
        "\n",
        "failed = []\n",
        "total_saved = 0\n",
        "skipped = 0\n",
        "retried = 0\n",
        "\n",
        "for state_name, iso_code in US_STATES.items():\n",
        "    out_path = state_outfile(iso_code)\n",
        "\n",
        "    existing = load_state_file_if_ok(out_path)\n",
        "    if existing is not None:\n",
        "        print(f\"Skip {state_name} (exists): {len(existing)}\")\n",
        "        total_saved += len(existing)\n",
        "        skipped += 1\n",
        "        continue\n",
        "    else:\n",
        "        if os.path.exists(out_path):\n",
        "            print(f\"Re-try {state_name} (file exists but empty/unreadable).\")\n",
        "            retried += 1\n",
        "\n",
        "    print(f\"Fetching data for {state_name}...\")\n",
        "    elems = get_walmarts_by_state(state_name, iso_code)\n",
        "\n",
        "    if elems is None:\n",
        "        failed.append(state_name)\n",
        "        continue\n",
        "\n",
        "    # Add state reference tag\n",
        "    for item in elems:\n",
        "        if isinstance(item, dict) and \"tags\" in item and isinstance(item[\"tags\"], dict):\n",
        "            item[\"tags\"][\"state_ref\"] = state_name\n",
        "\n",
        "    # Save\n",
        "    atomic_write_json(out_path, elems)\n",
        "\n",
        "    print(f\"  Done. Found {len(elems)} items.\")\n",
        "    total_saved += len(elems)\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "print(\"\\nFETCH PHASE DONE.\")\n",
        "print(\"Skipped states:\", skipped)\n",
        "print(\"Retried states:\", retried)\n",
        "print(\"Total saved (counted from files + new fetch):\", total_saved)\n",
        "print(\"FAILED states:\", failed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jupZGSPB9Bos",
        "outputId": "7c80891b-a01e-4d61-b775-e8e1e89e29f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found files: 51\n",
            "Reading walmart_US-AK_2024.json | 11 items\n",
            "Reading walmart_US-AL_2024.json | 199 items\n",
            "Reading walmart_US-AR_2024.json | 184 items\n",
            "Reading walmart_US-AZ_2024.json | 219 items\n",
            "Reading walmart_US-CA_2024.json | 431 items\n",
            "Reading walmart_US-CO_2024.json | 179 items\n",
            "Reading walmart_US-CT_2024.json | 42 items\n",
            "Reading walmart_US-DC_2024.json | 2 items\n",
            "Reading walmart_US-DE_2024.json | 10 items\n",
            "Reading walmart_US-FL_2024.json | 515 items\n",
            "Reading walmart_US-GA_2024.json | 296 items\n",
            "Reading walmart_US-HI_2024.json | 12 items\n",
            "Reading walmart_US-IA_2024.json | 82 items\n",
            "Reading walmart_US-ID_2024.json | 53 items\n",
            "Reading walmart_US-IL_2024.json | 237 items\n",
            "Reading walmart_US-IN_2024.json | 160 items\n",
            "Reading walmart_US-KS_2024.json | 110 items\n",
            "Reading walmart_US-KY_2024.json | 130 items\n",
            "Reading walmart_US-LA_2024.json | 180 items\n",
            "Reading walmart_US-MA_2024.json | 65 items\n",
            "Reading walmart_US-MD_2024.json | 68 items\n",
            "Reading walmart_US-ME_2024.json | 43 items\n",
            "Reading walmart_US-MI_2024.json | 121 items\n",
            "Reading walmart_US-MN_2024.json | 101 items\n",
            "Reading walmart_US-MO_2024.json | 202 items\n",
            "Reading walmart_US-MS_2024.json | 114 items\n",
            "Reading walmart_US-MT_2024.json | 36 items\n",
            "Reading walmart_US-NC_2024.json | 274 items\n",
            "Reading walmart_US-ND_2024.json | 16 items\n",
            "Reading walmart_US-NE_2024.json | 72 items\n",
            "Reading walmart_US-NH_2024.json | 47 items\n",
            "Reading walmart_US-NJ_2024.json | 76 items\n",
            "Reading walmart_US-NM_2024.json | 77 items\n",
            "Reading walmart_US-NV_2024.json | 69 items\n",
            "Reading walmart_US-NY_2024.json | 146 items\n",
            "Reading walmart_US-OH_2024.json | 265 items\n",
            "Reading walmart_US-OK_2024.json | 195 items\n",
            "Reading walmart_US-OR_2024.json | 105 items\n",
            "Reading walmart_US-PA_2024.json | 238 items\n",
            "Reading walmart_US-RI_2024.json | 16 items\n",
            "Reading walmart_US-SC_2024.json | 172 items\n",
            "Reading walmart_US-SD_2024.json | 29 items\n",
            "Reading walmart_US-TN_2024.json | 207 items\n",
            "Reading walmart_US-TX_2024.json | 860 items\n",
            "Reading walmart_US-UT_2024.json | 102 items\n",
            "Reading walmart_US-VA_2024.json | 218 items\n",
            "Reading walmart_US-VT_2024.json | 7 items\n",
            "Reading walmart_US-WA_2024.json | 139 items\n",
            "Reading walmart_US-WI_2024.json | 134 items\n",
            "Reading walmart_US-WV_2024.json | 56 items\n",
            "Reading walmart_US-WY_2024.json | 27 items\n",
            "\n",
            "Total after simple merge: 7349\n",
            "Saved merged JSON to: /content/drive/MyDrive/walmart_OSM/walmart_2024_all_states.json\n"
          ]
        }
      ],
      "source": [
        "# merge 51 state\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "\n",
        "IN_DIR = \"/content/drive/MyDrive/walmart_OSM/raw_2024_by_state\"\n",
        "OUT_PATH = \"/content/drive/MyDrive/walmart_OSM/walmart_2024_all_states.json\"\n",
        "\n",
        "all_walmarts = []\n",
        "\n",
        "paths = sorted(glob.glob(os.path.join(IN_DIR, \"walmart_US-*_2024.json\")))\n",
        "print(\"Found files:\", len(paths))\n",
        "\n",
        "for p in paths:\n",
        "    with open(p, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"Reading {os.path.basename(p)} | {len(data)} items\")\n",
        "    all_walmarts.extend(data)\n",
        "\n",
        "print(\"\\nTotal after simple merge:\", len(all_walmarts))\n",
        "\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump(all_walmarts, f)\n",
        "\n",
        "print(\"Saved merged JSON to:\", OUT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2019 Dataset:** Originally, an OSM 2019 temporal query was attempted with the same approach. However, due to the crowdsourced nature of OpenStreetMap, historical data density in 2019 was found to be insufficient for a reliable longitudinal study.\n",
        "\n",
        "**Replacement Source**: A high-fidelity Kaggle Walmart locations Dataset (as of November 2018) was utilized as the baseline for the 2019-era comparison."
      ],
      "metadata": {
        "id": "ljNbpBCFtxzL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWMdhqYkXwgl"
      },
      "source": [
        "## **2. Data Cleaning (2024)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Transformation:** Flattening Hierarchical OSM JSON into a Tabular Store Panel"
      ],
      "metadata": {
        "id": "jbZsOIfltlwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7O-C8Ls-cz7",
        "outputId": "0a94a614-4106-4b1a-fa59-81919a036715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 7349\n",
            "Missing coords: 0\n",
            "        osm_id osm_type        lat         lon  \\\n",
            "0  12097718365     node  61.192524 -149.880676   \n",
            "1  12097718366     node  61.191960 -149.880678   \n",
            "2     28615011      way  61.192314 -149.879649   \n",
            "3    170069699      way  55.375440 -131.720547   \n",
            "4    209450680      way  57.811955 -152.365307   \n",
            "\n",
            "                                 name    brand operator         street  \\\n",
            "0            Walmart Grocery Entrance     None     None           None   \n",
            "1  Walmart Home and Pharmacy Entrance     None     None           None   \n",
            "2                 Walmart Supercenter  Walmart  Walmart       A Street   \n",
            "3                             Walmart  Walmart  Walmart  Don King Road   \n",
            "4                             Walmart  Walmart  Walmart  Mill Bay Road   \n",
            "\n",
            "        city state    zip state_ref  \n",
            "0       None  None   None    Alaska  \n",
            "1       None  None   None    Alaska  \n",
            "2  Anchorage    AK  99503    Alaska  \n",
            "3  Ketchikan    AK  99901    Alaska  \n",
            "4     Kodiak    AK  99615    Alaska  \n"
          ]
        }
      ],
      "source": [
        "# Load the raw JSON data\n",
        "\n",
        "with open('/content/drive/MyDrive/walmart_OSM/walmart_2024_all_states.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "rows = []\n",
        "\n",
        "for item in data:\n",
        "    t = item.get('tags', {})\n",
        "\n",
        "    if item.get('type') == 'node':\n",
        "        lat = item.get('lat')\n",
        "        lon = item.get('lon')\n",
        "    else:\n",
        "        center = item.get('center', {})\n",
        "        lat = center.get('lat')\n",
        "        lon = center.get('lon')\n",
        "\n",
        "    rows.append({\n",
        "        'osm_id': item.get('id'),\n",
        "        'osm_type': item.get('type'),\n",
        "\n",
        "        'lat': lat,\n",
        "        'lon': lon,\n",
        "\n",
        "        'name': t.get('name'),\n",
        "        'brand': t.get('brand'),\n",
        "        'operator': t.get('operator'),\n",
        "\n",
        "        'street': t.get('addr:street'),\n",
        "        'city': t.get('addr:city'),\n",
        "        'state': t.get('addr:state'),\n",
        "        'zip': t.get('addr:postcode'),\n",
        "\n",
        "        'state_ref': t.get('state_ref'),\n",
        "    })\n",
        "\n",
        "df_2024 = pd.DataFrame(rows)\n",
        "\n",
        "print(\"Total rows:\", len(df_2024))\n",
        "print(\"Missing coords:\", df_2024['lat'].isna().sum())\n",
        "print(df_2024.head())\n",
        "\n",
        "df_2024.to_csv('/content/drive/MyDrive/walmart_OSM/walmart_2024_all_states.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tOt_LnoK5Po"
      },
      "source": [
        "### **Data Enrichment:** Geospatial Gap Filling via Reverse Geocoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems**: The initial OpenStreetMap (OSM) extraction contained inconsistent address metadata; several store records lacked critical geographic identifiers (City, State, or ZIP code), which are required to join the store data with Census ZCTA demographics.\n",
        "\n",
        "**Solutions:** We implemented a Reverse Geocoding pipeline using the Nominatim engine and the geopy library, targeting only the subset of records with missing address values. To ensure ethical API usage and pipeline stability, we utilized a RateLimiter to enforce a mandatory 1-second delay and integrated a checkpointing system that commits progress to a CSV every 200 records. This design prevents data loss and avoids redundant server requests in the event of a network timeout.\n",
        "\n",
        "**Expectations**: A fully enriched dataset where all store locations are mapped to their respective municipalities and ZIP codes, enabling a high-precision spatial merge with the socioeconomic baseline."
      ],
      "metadata": {
        "id": "xIL9dIzkvjn5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcZsN11HrWIA",
        "outputId": "62010a21-44d2-4725-cc55-8e1f7b663ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows needing reverse geocode: 2042 out of 7349\n",
            "Checkpoint saved: 200/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 400/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 600/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 800/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 1000/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 1200/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 1400/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 1600/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 1800/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Checkpoint saved: 2000/2042 -> /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n",
            "Done. Saved: /content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\n"
          ]
        }
      ],
      "source": [
        "# Portions of this code were generated with the assistance of OpenAI GPT-4\n",
        "\n",
        "# Summary of Key Prompts: To build a reliable enrichment pipeline, we used AI to address the following technical challenges:\n",
        "# 1. \"Write a script that performs reverse geocoding on rows where 'city', 'state', or 'zip' are missing.\"\n",
        "# 2. \"The Nominatim API has strict usage limits. How can I implement a RateLimiter that waits between requests and handles potential server timeouts or exceptions?\"\n",
        "# 3. \"How can I add a checkpoint system to this loop so that if the internet disconnects, I don't lose the geocoding progress?\"\n",
        "\n",
        "# Generated on: February 2026\n",
        "\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "\n",
        "df_2024 = pd.read_csv(\"/content/drive/MyDrive/walmart_OSM/walmart_2024_all_states.csv\")\n",
        "\n",
        "# pick id column name\n",
        "id_col = \"osm_id\" if \"osm_id\" in df_2024.columns else (\"id\" if \"id\" in df_2024.columns else None)\n",
        "if id_col is None:\n",
        "    raise ValueError(\"Cannot find an id column (expected 'osm_id' or 'id').\")\n",
        "\n",
        "# setup nominatim\n",
        "geolocator = Nominatim(user_agent=\"walmart_geocoder_research (contact: zhutt@umich.edu)\")\n",
        "\n",
        "# wrapper\n",
        "reverse = RateLimiter(\n",
        "    geolocator.reverse,\n",
        "    min_delay_seconds=1,\n",
        "    max_retries=2,\n",
        "    error_wait_seconds=5,\n",
        "    swallow_exceptions=False\n",
        ")\n",
        "\n",
        "# only geocode missing rows\n",
        "need = df_2024[\"city\"].isna() | df_2024[\"state\"].isna() | df_2024[\"zip\"].isna()\n",
        "to_fill_idx = df_2024.index[need].tolist()\n",
        "print(\"Rows needing reverse geocode:\", len(to_fill_idx), \"out of\", len(df_2024))\n",
        "\n",
        "def fill_one(idx):\n",
        "    row = df_2024.loc[idx]\n",
        "    if pd.isna(row[\"lat\"]) or pd.isna(row[\"lon\"]):\n",
        "        return  # can't reverse geocode without coords\n",
        "\n",
        "    try:\n",
        "        loc = reverse((row[\"lat\"], row[\"lon\"]), timeout=10)\n",
        "        if not loc:\n",
        "            return\n",
        "        addr = (loc.raw or {}).get(\"address\", {})\n",
        "\n",
        "        # fill city\n",
        "        if pd.isna(row[\"city\"]):\n",
        "            df_2024.at[idx, \"city\"] = addr.get(\"city\") or addr.get(\"town\") or addr.get(\"village\") or addr.get(\"hamlet\")\n",
        "\n",
        "        # fill state\n",
        "        if pd.isna(row[\"state\"]):\n",
        "            df_2024.at[idx, \"state\"] = addr.get(\"state_code\") or addr.get(\"state\")\n",
        "\n",
        "        # fill zip\n",
        "        if pd.isna(row[\"zip\"]):\n",
        "            df_2024.at[idx, \"zip\"] = addr.get(\"postcode\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error at {id_col}={row.get(id_col)} idx={idx}: {e}\")\n",
        "\n",
        "# Checkpoint\n",
        "OUT_PATH = \"/content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\"\n",
        "SAVE_EVERY = 200\n",
        "\n",
        "for n, idx in enumerate(to_fill_idx, 1):\n",
        "    fill_one(idx)\n",
        "    if n % SAVE_EVERY == 0:\n",
        "        df_2024.to_csv(OUT_PATH, index=False)\n",
        "        print(f\"Checkpoint saved: {n}/{len(to_fill_idx)} -> {OUT_PATH}\")\n",
        "\n",
        "# final save\n",
        "df_2024.to_csv(OUT_PATH, index=False)\n",
        "print(\"Done. Saved:\", OUT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VinsgkBdLBtT"
      },
      "source": [
        "### **Data Standardization:** Refinement and Structural Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems:** The enriched dataset still contained \"noisy\" data, including non-Supercenter locations (like Fuel Station and Garden Cneter), non-standardized ZIP codes (e.g., ZIP+4 formats), and inconsistent state naming conventions (e.g., \"Florida\" vs. \"FL\").\n",
        "\n",
        "**Solutions**:  We applied a strict structural normalization process that filtered for a uniform business entity, parsed geographic identifiers into a standard 5-digit format, and mapped state names to official two-letter ISO abbreviations  Standardizing these keys is vital for the longitudinal merge. Without this step, a store recorded as \"FL\" in 2019 and \"Florida\" in 2024 would appear as two different entities, or a store with a 9-digit ZIP code would fail to join with the 5-digit Census ZCTA data.\n",
        "\n",
        "**Expectations:** A cleaned dataset of Walmart Supercenters with standardized geographic variables, ensuring an error-free integration with both the 2019 baseline and the ACS Census demographic layers."
      ],
      "metadata": {
        "id": "hMukDLbtwnhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OVvnyi6QMk7",
        "outputId": "ac5fe104-f5bc-4736-8ee8-a102c6b0e21c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         lat         lon                 name         city state    zip\n",
            "2  61.192314 -149.879649  Walmart Supercenter    Anchorage    AK  99503\n",
            "5  64.856394 -147.689449  Walmart Supercenter    Fairbanks    AK  99701\n",
            "6  61.309108 -149.535586  Walmart Supercenter  Eagle River    AK  99577\n",
            "7  60.564179 -151.225002  Walmart Supercenter        Kenai    AK  99611\n",
            "8  61.568783 -149.365026  Walmart Supercenter      Wasilla    AK  99654\n"
          ]
        }
      ],
      "source": [
        "df_2024 = pd.read_csv(\"/content/drive/MyDrive/walmart_OSM/walmart_2024_geocode.csv\")\n",
        "# Keep only the requested columns\n",
        "# cols_to_keep = ['id', 'name', 'zip', 'state']\n",
        "# df_2024 = df_2024[cols_to_keep]\n",
        "\n",
        "# Filter for Walmart Supercenter ONLY\n",
        "target_names = ['Walmart Supercenter']\n",
        "df_2024 = df_2024[df_2024['name'].isin(target_names)]\n",
        "\n",
        "df_2024 = df_2024.dropna(subset=['zip'])\n",
        "\n",
        "# Extract only the first ZIP code\n",
        "# This splits by commas, spaces, or hyphens and takes the first part\n",
        "df_2024['zip'] = df_2024['zip'].astype(str).str.split(r'[;\\s\\-]').str[0]\n",
        "\n",
        "# Final safety check to remove any empty or invalid strings\n",
        "df_2024 = df_2024[df_2024['zip'] != 'nan']\n",
        "df_2024 = df_2024[df_2024['zip'].str.strip() != '']\n",
        "\n",
        "df_2024 = df_2024[['lat', 'lon','name', 'city', 'state', 'zip']]\n",
        "\n",
        "us_state_to_abbrev = {\n",
        "    \"ALABAMA\":\"AL\",\"ALASKA\":\"AK\",\"ARIZONA\":\"AZ\",\"ARKANSAS\":\"AR\",\"CALIFORNIA\":\"CA\",\n",
        "    \"COLORADO\":\"CO\",\"CONNECTICUT\":\"CT\",\"DELAWARE\":\"DE\",\"DISTRICT OF COLUMBIA\":\"DC\",\n",
        "    \"FLORIDA\":\"FL\",\"GEORGIA\":\"GA\",\"HAWAII\":\"HI\",\"IDAHO\":\"ID\",\"ILLINOIS\":\"IL\",\n",
        "    \"INDIANA\":\"IN\",\"IOWA\":\"IA\",\"KANSAS\":\"KS\",\"KENTUCKY\":\"KY\",\"LOUISIANA\":\"LA\",\n",
        "    \"MAINE\":\"ME\",\"MARYLAND\":\"MD\",\"MASSACHUSETTS\":\"MA\",\"MICHIGAN\":\"MI\",\n",
        "    \"MINNESOTA\":\"MN\",\"MISSISSIPPI\":\"MS\",\"MISSOURI\":\"MO\",\"MONTANA\":\"MT\",\n",
        "    \"NEBRASKA\":\"NE\",\"NEVADA\":\"NV\",\"NEW HAMPSHIRE\":\"NH\",\"NEW JERSEY\":\"NJ\",\n",
        "    \"NEW MEXICO\":\"NM\",\"NEW YORK\":\"NY\",\"NORTH CAROLINA\":\"NC\",\"NORTH DAKOTA\":\"ND\",\n",
        "    \"OHIO\":\"OH\",\"OKLAHOMA\":\"OK\",\"OREGON\":\"OR\",\"PENNSYLVANIA\":\"PA\",\n",
        "    \"RHODE ISLAND\":\"RI\",\"SOUTH CAROLINA\":\"SC\",\"SOUTH DAKOTA\":\"SD\",\n",
        "    \"TENNESSEE\":\"TN\",\"TEXAS\":\"TX\",\"UTAH\":\"UT\",\"VERMONT\":\"VT\",\"VIRGINIA\":\"VA\",\n",
        "    \"WASHINGTON\":\"WA\",\"WEST VIRGINIA\":\"WV\",\"WISCONSIN\":\"WI\",\"WYOMING\":\"WY\",\n",
        "    \"PUERTO RICO\":\"PR\"\n",
        "}\n",
        "\n",
        "def clean_state(df):\n",
        "    df = df.copy()\n",
        "    df[\"state\"] = df[\"state\"].str.upper().str.strip()\n",
        "    df[\"state\"] = df[\"state\"].replace(us_state_to_abbrev)\n",
        "    return df\n",
        "\n",
        "df_2024 = clean_state(df_2024)\n",
        "\n",
        "df_2024.to_csv('/content/drive/MyDrive/walmart_OSM/walmart_2024_final.csv', index=False)\n",
        "\n",
        "print(df_2024.head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPhPp0OrPbid"
      },
      "source": [
        "## **3. Data Cleaning (2019)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li0-vr_VTQIG"
      },
      "outputs": [],
      "source": [
        "df_2019 = pd.read_csv(\"/content/drive/MyDrive/walmart_OSM/walmart_Kaggle.csv\")\n",
        "df_2019 = df_2019.rename(columns={'zip_code': 'zip', 'longitude':'lon', 'latitude': 'lat'})\n",
        "\n",
        "df_2019 = df_2019[['name', 'city', 'state', 'zip', 'lat', 'lon']]\n",
        "df_2019 = df_2019[df_2019['name'].str.contains('Supercenter', na=False)]\n",
        "\n",
        "df_2019 = clean_state(df_2019)\n",
        "\n",
        "df_2019.to_csv('/content/drive/MyDrive/walmart_OSM/walmart_Kaggle_final.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ue_ujJHLuPA"
      },
      "source": [
        "## **4. Store Status Comparison by ZIP Code (2019-2024)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-Check**"
      ],
      "metadata": {
        "id": "_GQvz_EJ0R9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-check\n",
        "df_2019 = df_2019.copy()\n",
        "df_2024 = df_2024.copy()\n",
        "\n",
        "df_2019[\"state\"] = df_2019[\"state\"].str.upper().str.strip()\n",
        "df_2024[\"state\"] = df_2024[\"state\"].str.upper().str.strip()\n",
        "\n",
        "count_2019 = (\n",
        "    df_2019.groupby(\"state\")\n",
        "    .size()\n",
        "    .reset_index(name=\"stores_2019\")\n",
        ")\n",
        "\n",
        "count_2024 = (\n",
        "    df_2024.groupby(\"state\")\n",
        "    .size()\n",
        "    .reset_index(name=\"stores_2024\")\n",
        ")\n",
        "\n",
        "print(count_2019)\n",
        "print(count_2024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXPoEdhalbo_",
        "outputId": "85e44737-bdc3-4ba8-a0ae-b98c6477da49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   state  stores_2019\n",
            "0     AK            7\n",
            "1     AL          101\n",
            "2     AR           76\n",
            "3     AZ           84\n",
            "4     CA          141\n",
            "5     CO           69\n",
            "6     CT           12\n",
            "7     DC            3\n",
            "8     DE            6\n",
            "9     FL          228\n",
            "10    GA          154\n",
            "11    IA           58\n",
            "12    ID           23\n",
            "13    IL          138\n",
            "14    IN           97\n",
            "15    KS           58\n",
            "16    KY           78\n",
            "17    LA           89\n",
            "18    MA           26\n",
            "19    MD           29\n",
            "20    ME           19\n",
            "21    MI           91\n",
            "22    MN           65\n",
            "23    MO          112\n",
            "24    MS           65\n",
            "25    MT           14\n",
            "26    NC          142\n",
            "27    ND           14\n",
            "28    NE           35\n",
            "29    NH           19\n",
            "30    NJ           32\n",
            "31    NM           35\n",
            "32    NV           30\n",
            "33    NY           80\n",
            "34    OH          138\n",
            "35    OK           81\n",
            "36    OR           28\n",
            "37    PA          116\n",
            "38    PR           13\n",
            "39    RI            5\n",
            "40    SC           83\n",
            "41    SD           15\n",
            "42    TN          117\n",
            "43    TX          387\n",
            "44    UT           41\n",
            "45    VA          110\n",
            "46    VT            3\n",
            "47    WA           52\n",
            "48    WI           83\n",
            "49    WV           38\n",
            "50    WY           12\n",
            "   state  stores_2024\n",
            "0     AK            7\n",
            "1     AL          103\n",
            "2     AR           75\n",
            "3     AZ           84\n",
            "4     CA          145\n",
            "5     CO           71\n",
            "6     CT           13\n",
            "7     DC            2\n",
            "8     DE            6\n",
            "9     FL          237\n",
            "10    GA          159\n",
            "11    HI            1\n",
            "12    IA           59\n",
            "13    ID           23\n",
            "14    IL          141\n",
            "15    IN           96\n",
            "16    KS           58\n",
            "17    KY           77\n",
            "18    LA           88\n",
            "19    MA           28\n",
            "20    MD           31\n",
            "21    ME           19\n",
            "22    MI           90\n",
            "23    MN           64\n",
            "24    MO          112\n",
            "25    MS           68\n",
            "26    MT           14\n",
            "27    NC          148\n",
            "28    ND           14\n",
            "29    NE           35\n",
            "30    NH           19\n",
            "31    NJ           35\n",
            "32    NM           34\n",
            "33    NV           30\n",
            "34    NY           83\n",
            "35    OH          138\n",
            "36    OK           88\n",
            "37    OR           27\n",
            "38    PA          116\n",
            "39    RI            5\n",
            "40    SC           83\n",
            "41    SD           15\n",
            "42    TN          119\n",
            "43    TX          392\n",
            "44    UT           41\n",
            "45    VA          110\n",
            "46    VT            3\n",
            "47    WA           52\n",
            "48    WI           84\n",
            "49    WV           38\n",
            "50    WY           12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Merge:** Spatial Join & Store Status Classification"
      ],
      "metadata": {
        "id": "hcj-pwcl0p7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2NCu2P6NOOw",
        "outputId": "25895615-5910-4ff4-ad40-d24838441c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "store_status\n",
            "existing        3548\n",
            "newly_opened     229\n",
            "closed            88\n",
            "Name: count, dtype: int64\n",
            "store_status_code\n",
            "1    3548\n",
            "2     229\n",
            "0      88\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def normalize_keys(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    df[\"city\"]  = df[\"city\"].astype(\"string\").str.upper().str.strip()\n",
        "    df[\"state\"] = df[\"state\"].astype(\"string\").str.upper().str.strip()\n",
        "    z = df[\"zip\"].astype(\"string\").str.strip()\n",
        "    z = z.str.extract(r\"(\\d{5})\", expand=False)\n",
        "    df[\"zip\"] = z.str.zfill(5)\n",
        "    df = df[df[\"zip\"].notna() & (df[\"zip\"].str.len() == 5)].copy()\n",
        "\n",
        "    return df\n",
        "\n",
        "df_2019 = normalize_keys(df_2019)\n",
        "df_2024 = normalize_keys(df_2024)\n",
        "\n",
        "# merge\n",
        "keys = [\"zip\", \"city\", \"state\"]\n",
        "\n",
        "merged = df_2019.merge(\n",
        "    df_2024,\n",
        "    on=keys,\n",
        "    how=\"outer\",\n",
        "    indicator=True,\n",
        "    suffixes=(\"_2019\", \"_2024\")\n",
        ")\n",
        "\n",
        "def classify_status(row):\n",
        "    if row[\"_merge\"] == \"left_only\":\n",
        "        return \"closed\"\n",
        "    elif row[\"_merge\"] == \"both\":\n",
        "        return \"existing\"\n",
        "    else:\n",
        "        return \"newly_opened\"\n",
        "\n",
        "merged[\"store_status\"] = merged.apply(classify_status, axis=1)\n",
        "\n",
        "status_map = {\n",
        "    \"closed\": 0,\n",
        "    \"existing\": 1,\n",
        "    \"newly_opened\": 2\n",
        "}\n",
        "\n",
        "merged[\"store_status_code\"] = merged[\"store_status\"].map(status_map)\n",
        "\n",
        "\n",
        "print(merged[\"store_status\"].value_counts())\n",
        "print(merged[\"store_status_code\"].value_counts())\n",
        "\n",
        "merged.to_csv(\"/content/drive/MyDrive/walmart_OSM/walmart_closure_comparison.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Post-Audit**"
      ],
      "metadata": {
        "id": "Fo211_8O36YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"zip not 5 digits:\",\n",
        "      (~merged[\"zip\"].astype(str).str.match(r\"^\\d{5}$\")).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmD0_-na7gDt",
        "outputId": "fa2c772b-53cf-49e6-b664-bd93f9e0d5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zip not 5 digits: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"zip+city+state\" cause duplicate\n",
        "keys = [\"zip\",\"city\",\"state\"]\n",
        "\n",
        "dup19 = df_2019.duplicated(keys, keep=False).sum()\n",
        "dup24 = df_2024.duplicated(keys, keep=False).sum()\n",
        "\n",
        "print(\"2019 duplicate key rows:\", dup19)\n",
        "print(\"2024 duplicate key rows:\", dup24)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvpS2DJrpeJX",
        "outputId": "6cf4a0cf-33e9-42f2-def0-f13a42b6078f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2019 duplicate key rows: 177\n",
            "2024 duplicate key rows: 259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for label in [\"closed\",\"newly_opened\"]:\n",
        "    tmp = merged[merged[\"store_status\"] == label]\n",
        "    multi = tmp.groupby(keys).size().sort_values(ascending=False).head(10)\n",
        "    print(\"\\nTop duplicated keys in\", label)\n",
        "    print(multi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b63Qsmp0pqhI",
        "outputId": "842bff4a-1915-449b-da8e-19db7ea97cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top duplicated keys in closed\n",
            "zip    city              state\n",
            "14043  LANCASTER         NY       1\n",
            "14221  WILLIAMSVILLE     NY       1\n",
            "14225  CHEEKTOWAGA       NY       1\n",
            "14616  GREECE            NY       1\n",
            "15005  ECONOMY           PA       1\n",
            "15417  WEST BROWNSVILLE  PA       1\n",
            "17319  ETTERS            PA       1\n",
            "20001  WASHINGTON        DC       1\n",
            "20176  LEESBURG          VA       1\n",
            "21826  SALISBURY         MD       1\n",
            "dtype: int64\n",
            "\n",
            "Top duplicated keys in newly_opened\n",
            "zip    city             state\n",
            "74136  TULSA            OK       6\n",
            "27858  EDWARDS ACRES    NC       3\n",
            "62707  SPRINGFIELD      IL       3\n",
            "01085  WESTFIELD        MA       2\n",
            "35642  GULF SHORES      AL       2\n",
            "32222  JACKSONVILLE     FL       2\n",
            "33177  MIAMI            FL       2\n",
            "33962  WINDING CYPRESS  FL       2\n",
            "28584  SWANSBORO        NC       2\n",
            "02767  RAYNHAM          MA       2\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nojC20kVi3hY"
      },
      "source": [
        "# **Secondary Data: American Community Survey (ACS)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Data Cleaning**"
      ],
      "metadata": {
        "id": "yDne1kyCeQa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Standardization:** Median Income"
      ],
      "metadata": {
        "id": "B4HPbmAf63kT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PoC_wZFj2q3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_income = pd.read_csv(\"/content/drive/MyDrive/walmart_OSM/B19013 Median household income.csv\",skiprows=[1])\n",
        "df_income.columns = df_income.columns.str.lower()\n",
        "df_income[\"zipcode\"] = df_income[\"geo_id\"].str[-5:]\n",
        "\n",
        "df_income = df_income[[\"zipcode\", \"b19013_001e\"]]\n",
        "df_income = df_income.rename(columns={\n",
        "    \"b19013_001e\": \"median_income\"\n",
        "})\n",
        "\n",
        "df_income[\"median_income\"] = pd.to_numeric(df_income[\"median_income\"], errors=\"coerce\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Standardization:** Total Population"
      ],
      "metadata": {
        "id": "8GTEy70w7Y2j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSxxciH_lQ8n"
      },
      "outputs": [],
      "source": [
        "df_pop = pd.read_csv(\"/content/drive/MyDrive/walmart_OSM/B01003 Total population.csv\",skiprows=[1])\n",
        "df_pop.columns = df_pop.columns.str.lower()\n",
        "df_pop[\"zipcode\"] = df_pop[\"geo_id\"].str[-5:]\n",
        "\n",
        "df_pop = df_pop[[\"zipcode\", \"b01003_001e\"]]\n",
        "df_pop = df_pop.rename(columns={\n",
        "    \"b01003_001e\": \"total_population\"\n",
        "})\n",
        "\n",
        "df_pop[\"total_population\"] = pd.to_numeric(df_pop[\"total_population\"], errors=\"coerce\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Standardization:** Hispanic Share"
      ],
      "metadata": {
        "id": "Qg3beVaA7evv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dOtJSw-lS_-"
      },
      "outputs": [],
      "source": [
        "df_race = pd.read_csv(\"/content/drive/MyDrive/walmart_OSM/B03002 Hispanic or Latino population.csv\",skiprows=[1])\n",
        "df_race.columns = df_race.columns.str.lower()\n",
        "df_race[\"zipcode\"] = df_race[\"geo_id\"].str[-5:]\n",
        "df_race = df_race[[\n",
        "    \"zipcode\",\n",
        "    \"b03002_012e\",  # Hispanic\n",
        "    \"b03002_001e\"   # Total\n",
        "]]\n",
        "\n",
        "df_race[\"b03002_012e\"] = pd.to_numeric(df_race[\"b03002_012e\"], errors=\"coerce\")\n",
        "df_race[\"b03002_001e\"] = pd.to_numeric(df_race[\"b03002_001e\"], errors=\"coerce\")\n",
        "\n",
        "df_race[\"hispanic_share\"] = (\n",
        "    df_race[\"b03002_012e\"] / df_race[\"b03002_001e\"]\n",
        ")\n",
        "\n",
        "df_race = df_race[[\"zipcode\", \"hispanic_share\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Merge:** Multi-Variable Join of Census Attributes"
      ],
      "metadata": {
        "id": "5GJ6DPZA7vDT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMt2er2zlWNc"
      },
      "outputs": [],
      "source": [
        "acs_master = (\n",
        "    df_income\n",
        "    .merge(df_pop, on=\"zipcode\", how=\"left\")\n",
        "    .merge(df_race, on=\"zipcode\", how=\"left\")\n",
        ")\n",
        "\n",
        "acs_master = acs_master.dropna().reset_index(drop=True)\n",
        "\n",
        "acs_master.to_csv(\n",
        "    \"/content/drive/MyDrive/walmart_OSM/walmart_acs_master.csv\",\n",
        "    index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoOw79tIH5p4"
      },
      "source": [
        "## **2. Merge**: Spatial Aggregation and Socioeconomic Data Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-Check**"
      ],
      "metadata": {
        "id": "Qtwh8DYX-DK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hq_TGnKIJoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02447c8a-f6c5-4434-b586-4f42b69c09dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "closure cols: ['name_2019', 'city', 'state', 'zipcode', 'lat_2019', 'lon_2019', 'lat_2024', 'lon_2024', 'name_2024', '_merge', 'store_status', 'store_status_code']\n",
            "acs cols: ['zipcode', 'median_income', 'total_population', 'hispanic_share']\n"
          ]
        }
      ],
      "source": [
        "closure_path = \"/content/drive/MyDrive/walmart_OSM/walmart_closure_comparison.csv\"\n",
        "acs_path     = \"/content/drive/MyDrive/walmart_OSM/walmart_acs_master.csv\"\n",
        "\n",
        "df_closure = pd.read_csv(closure_path, dtype=str)\n",
        "df_acs     = pd.read_csv(acs_path, dtype=str)\n",
        "\n",
        "df_closure = df_closure.rename(columns={\"zip\": \"zipcode\"})\n",
        "\n",
        "print(\"closure cols:\", df_closure.columns.tolist())\n",
        "print(\"acs cols:\", df_acs.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Merge**: ACS and Walmart Locations at ZIP-leve"
      ],
      "metadata": {
        "id": "_Y7EEZjd-HOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAYyDX00WsRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e869bf-1a31-4bf8-e36b-5e96a0771adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  zipcode         city state store_status_code  store_status median_income  \\\n",
            "0   01020     CHICOPEE    MA                 2  newly_opened       70971.0   \n",
            "1   01082         WARE    MA                 2  newly_opened       71023.0   \n",
            "2   01085    WESTFIELD    MA                 2  newly_opened       83821.0   \n",
            "3   01119  SPRINGFIELD    MA                 2  newly_opened       59172.0   \n",
            "4   01247  NORTH ADAMS    MA                 2  newly_opened       55544.0   \n",
            "\n",
            "  total_population        hispanic_share  \n",
            "0            30230   0.20559047304002645  \n",
            "1            10418   0.05442503359569975  \n",
            "2            41466    0.0982732841364009  \n",
            "3            14059    0.3316736610000711  \n",
            "4            14967  0.054653571189951226  \n",
            "ACS match rate: 0.9983074753173484\n"
          ]
        }
      ],
      "source": [
        "# closure to ZIP-level\n",
        "zip_level = (\n",
        "    df_closure\n",
        "        .groupby([\"zipcode\", \"city\", \"state\"], as_index=False)\n",
        "        .agg({\n",
        "            \"store_status_code\": \"min\"   # closed(0) < existing(1) < newly_opened(2)\n",
        "        })\n",
        ")\n",
        "\n",
        "zip_level[\"store_status\"] = zip_level[\"store_status_code\"].map({\n",
        "    \"0\": \"closed\",\n",
        "    \"1\": \"existing\",\n",
        "    \"2\": \"newly_opened\"\n",
        "})\n",
        "\n",
        "#  ACS to ZIP-level\n",
        "df_acs[\"zipcode\"] = (\n",
        "    df_acs[\"zipcode\"]\n",
        "      .astype(str)\n",
        "      .str.extract(r\"(\\d{5})\", expand=False)\n",
        "      .str.zfill(5)\n",
        ")\n",
        "\n",
        "# merge\n",
        "final = zip_level.merge(df_acs, on=\"zipcode\", how=\"left\")\n",
        "\n",
        "print(final.head())\n",
        "print(\"ACS match rate:\", final[\"median_income\"].notna().mean())\n",
        "\n",
        "final.to_csv(\n",
        "    \"/content/drive/MyDrive/walmart_OSM/walmart_closure_with_acs.csv\",\n",
        "    index=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Post-Audit**"
      ],
      "metadata": {
        "id": "5yTgANeY99L_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHGUHBplWyKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d448add-0e23-4d2c-946c-49886ae597b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "closure rows: 3865\n",
            "acs rows: 30509\n",
            "closure zipcode missing: 0\n",
            "ACS matched rows: 3539\n",
            "ACS match rate: 99.83%\n"
          ]
        }
      ],
      "source": [
        "print(\"closure rows:\", len(df_closure))\n",
        "print(\"acs rows:\", len(df_acs))\n",
        "print(\"closure zipcode missing:\", final[\"zipcode\"].isna().sum())\n",
        "print(\"ACS matched rows:\", final[\"median_income\"].notna().sum())\n",
        "print(\"ACS match rate:\", f'{final[\"median_income\"].notna().mean():.2%}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "283DLy7mi5wp",
        "YmN8s35Oi7Bn",
        "VpX5MZvdp-fy",
        "W7JiRTs9KHrw",
        "nWMdhqYkXwgl",
        "jbZsOIfltlwE",
        "0tOt_LnoK5Po",
        "VinsgkBdLBtT",
        "HPhPp0OrPbid",
        "1ue_ujJHLuPA",
        "_GQvz_EJ0R9M",
        "hcj-pwcl0p7r",
        "Fo211_8O36YS",
        "nojC20kVi3hY",
        "yDne1kyCeQa8",
        "NoOw79tIH5p4",
        "Qtwh8DYX-DK4",
        "_Y7EEZjd-HOO",
        "5yTgANeY99L_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}